{
  "name": "a",
  "version": "1.0.0",
  "description": "------------- 1.下载代码，npm install<br /> 2.node index.js即可<br /> 3.停止Cltr+c<br /> ## 博文 好的，我们从爬虫流程开始分析我们需要的一些模块。<br /> 首先，我们需要发送请求获得页面，在这里呢，我们用到了request-promise模块。<br /> <pre><code>     const rp = require(\"request-promise\"), //进入request-promise模块       async getPage(URL) {         const data = {           url, <br />           res: await rp(              url: URL           })        };        return data //这样，我们返回了一个对象，就是这个页面的url和页面内容。     } </code></pre> 其次，解析页面，我们使用一个叫做Cheerio的模块将上面返回的对象中的res解析成类似JQ的调用模式。Cheerio使用一个非常简单，一致的DOM模型。因此解析，操作和渲染非常高效。初步的端到端基准测试表明cheerio 比JSDOM快大约8倍。<br /> <pre><code>     const cheerio = require(\"cheerio\");//引入Cheerio模块     const $ = cheerio.load(data.res); //将html转换为可操作的节点 </code></pre>",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "dependencies": {
    "cheerio": "^1.0.0-rc.2",
    "fs": "0.0.1-security",
    "request": "^2.88.0",
    "request-promise": "^4.2.2"
  },
  "devDependencies": {},
  "repository": {
    "type": "git",
    "url": "git+https://github.com/proxygit/crawler.git"
  },
  "bugs": {
    "url": "https://github.com/proxygit/crawler.git/issues"
  },
  "homepage": "https://github.com/proxygit/crawler.git#readme"
}
